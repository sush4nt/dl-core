{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i9fzPfA_FnZB"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Importing libs"
      ],
      "metadata": {
        "id": "i9fzPfA_FnZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "pEAowgq_FmLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "metadata": {
        "id": "gWoitFHuF_pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Defining CNN"
      ],
      "metadata": {
        "id": "MBofii8JGG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi19-uk3GAdE",
        "outputId": "fa97ba1e-de12-45c9-c1f5-6c2df1cff6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using L2 regularization and Dropout to induce Regularization, so that we dont overfit to the data\n",
        "\n",
        "def my_model(give_summary=None):\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    x1 = layers.Conv2D(32, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01),)(\n",
        "        inputs\n",
        "    )\n",
        "    x1 = layers.BatchNormalization()(x1)\n",
        "    x1 = keras.activations.relu(x1)\n",
        "    x1 = layers.MaxPooling2D()(x1)\n",
        "\n",
        "    x2 = layers.Conv2D(64, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01),)(\n",
        "        x1\n",
        "    )\n",
        "    x2 = layers.BatchNormalization()(x1)\n",
        "    x2 = keras.activations.relu(x2)\n",
        "    x2 = layers.MaxPooling2D()(x2)\n",
        "\n",
        "    x3 = layers.Conv2D(\n",
        "        128, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01),\n",
        "    )(x2)\n",
        "    x3 = layers.BatchNormalization()(x3)\n",
        "    x3 = keras.activations.relu(x3)\n",
        "    x3 = layers.Flatten()(x3)\n",
        "\n",
        "    x4 = layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01),)(\n",
        "        x3\n",
        "    )\n",
        "    x4 = layers.Dropout(0.5)(x4)\n",
        "    outputs = layers.Dense(10)(x4)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    if give_summary:\n",
        "      print(model.summary())\n",
        "    else:\n",
        "      pass\n",
        "    return model\n",
        "my_model(give_summary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk7uxDukGRVE",
        "outputId": "7d39eab9-e8cf-4300-e458-184a51ab9c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " tf.nn.relu (TFOpLambda)     (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16, 16, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_1 (TFOpLambda)   (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         36992     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8, 8, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_2 (TFOpLambda)   (None, 8, 8, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8192)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                524352    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 563,658\n",
            "Trainable params: 563,274\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f88484c52e0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYv6DBQ6Fb2o",
        "outputId": "972f8443-5aa5-408b-c070-6701b423c30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 4s - loss: 2.6099 - accuracy: 0.3504 - 4s/epoch - 5ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 3s - loss: 1.8086 - accuracy: 0.4624 - 3s/epoch - 4ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 4s - loss: 1.6130 - accuracy: 0.4970 - 4s/epoch - 5ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 3s - loss: 1.5208 - accuracy: 0.5146 - 3s/epoch - 4ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 3s - loss: 1.4704 - accuracy: 0.5256 - 3s/epoch - 4ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 3s - loss: 1.4348 - accuracy: 0.5390 - 3s/epoch - 4ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 3s - loss: 1.4140 - accuracy: 0.5452 - 3s/epoch - 4ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 3s - loss: 1.4008 - accuracy: 0.5506 - 3s/epoch - 4ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 3s - loss: 1.3829 - accuracy: 0.5562 - 3s/epoch - 4ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 3s - loss: 1.3705 - accuracy: 0.5644 - 3s/epoch - 4ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 3s - loss: 1.3688 - accuracy: 0.5648 - 3s/epoch - 4ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 3s - loss: 1.3525 - accuracy: 0.5684 - 3s/epoch - 4ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 3s - loss: 1.3400 - accuracy: 0.5737 - 3s/epoch - 4ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 3s - loss: 1.3349 - accuracy: 0.5767 - 3s/epoch - 4ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 3s - loss: 1.3290 - accuracy: 0.5772 - 3s/epoch - 4ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 3s - loss: 1.3251 - accuracy: 0.5822 - 3s/epoch - 4ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 3s - loss: 1.3179 - accuracy: 0.5843 - 3s/epoch - 4ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 3s - loss: 1.3097 - accuracy: 0.5867 - 3s/epoch - 4ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 3s - loss: 1.3040 - accuracy: 0.5913 - 3s/epoch - 4ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 3s - loss: 1.2880 - accuracy: 0.5966 - 3s/epoch - 4ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 3s - loss: 1.2912 - accuracy: 0.5957 - 3s/epoch - 4ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 3s - loss: 1.2780 - accuracy: 0.6012 - 3s/epoch - 4ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 3s - loss: 1.2805 - accuracy: 0.6007 - 3s/epoch - 4ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 3s - loss: 1.2661 - accuracy: 0.6063 - 3s/epoch - 4ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 3s - loss: 1.2665 - accuracy: 0.6068 - 3s/epoch - 4ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 3s - loss: 1.2664 - accuracy: 0.6092 - 3s/epoch - 4ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 3s - loss: 1.2577 - accuracy: 0.6119 - 3s/epoch - 4ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 3s - loss: 1.2444 - accuracy: 0.6139 - 3s/epoch - 4ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 3s - loss: 1.2438 - accuracy: 0.6177 - 3s/epoch - 4ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 3s - loss: 1.2444 - accuracy: 0.6158 - 3s/epoch - 4ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 3s - loss: 1.2422 - accuracy: 0.6188 - 3s/epoch - 4ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 3s - loss: 1.2348 - accuracy: 0.6212 - 3s/epoch - 4ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 3s - loss: 1.2307 - accuracy: 0.6254 - 3s/epoch - 4ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 3s - loss: 1.2278 - accuracy: 0.6236 - 3s/epoch - 4ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 3s - loss: 1.2240 - accuracy: 0.6276 - 3s/epoch - 4ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 3s - loss: 1.2210 - accuracy: 0.6261 - 3s/epoch - 4ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 3s - loss: 1.2173 - accuracy: 0.6280 - 3s/epoch - 4ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 3s - loss: 1.2051 - accuracy: 0.6307 - 3s/epoch - 4ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 3s - loss: 1.2090 - accuracy: 0.6306 - 3s/epoch - 4ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 3s - loss: 1.2047 - accuracy: 0.6375 - 3s/epoch - 4ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 3s - loss: 1.1997 - accuracy: 0.6365 - 3s/epoch - 4ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 3s - loss: 1.2039 - accuracy: 0.6380 - 3s/epoch - 4ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 3s - loss: 1.1968 - accuracy: 0.6363 - 3s/epoch - 4ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 3s - loss: 1.1923 - accuracy: 0.6421 - 3s/epoch - 4ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 3s - loss: 1.1926 - accuracy: 0.6444 - 3s/epoch - 4ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 4s - loss: 1.1903 - accuracy: 0.6418 - 4s/epoch - 5ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 3s - loss: 1.1809 - accuracy: 0.6477 - 3s/epoch - 4ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 3s - loss: 1.1853 - accuracy: 0.6438 - 3s/epoch - 4ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 3s - loss: 1.1816 - accuracy: 0.6450 - 3s/epoch - 4ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 3s - loss: 1.1796 - accuracy: 0.6486 - 3s/epoch - 4ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 3s - loss: 1.1795 - accuracy: 0.6462 - 3s/epoch - 4ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 3s - loss: 1.1758 - accuracy: 0.6506 - 3s/epoch - 4ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 3s - loss: 1.1665 - accuracy: 0.6507 - 3s/epoch - 4ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 3s - loss: 1.1763 - accuracy: 0.6483 - 3s/epoch - 4ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 3s - loss: 1.1684 - accuracy: 0.6546 - 3s/epoch - 4ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 3s - loss: 1.1694 - accuracy: 0.6538 - 3s/epoch - 4ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 3s - loss: 1.1630 - accuracy: 0.6542 - 3s/epoch - 4ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 3s - loss: 1.1610 - accuracy: 0.6544 - 3s/epoch - 4ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 3s - loss: 1.1582 - accuracy: 0.6564 - 3s/epoch - 4ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 3s - loss: 1.1571 - accuracy: 0.6568 - 3s/epoch - 4ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 3s - loss: 1.1594 - accuracy: 0.6576 - 3s/epoch - 4ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 3s - loss: 1.1516 - accuracy: 0.6591 - 3s/epoch - 4ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 3s - loss: 1.1526 - accuracy: 0.6575 - 3s/epoch - 4ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 3s - loss: 1.1489 - accuracy: 0.6655 - 3s/epoch - 4ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 3s - loss: 1.1482 - accuracy: 0.6616 - 3s/epoch - 4ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 3s - loss: 1.1494 - accuracy: 0.6641 - 3s/epoch - 4ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 3s - loss: 1.1463 - accuracy: 0.6655 - 3s/epoch - 4ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 3s - loss: 1.1427 - accuracy: 0.6664 - 3s/epoch - 4ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 3s - loss: 1.1425 - accuracy: 0.6655 - 3s/epoch - 4ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 3s - loss: 1.1429 - accuracy: 0.6641 - 3s/epoch - 4ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 3s - loss: 1.1368 - accuracy: 0.6695 - 3s/epoch - 4ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 3s - loss: 1.1360 - accuracy: 0.6694 - 3s/epoch - 4ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 3s - loss: 1.1385 - accuracy: 0.6697 - 3s/epoch - 4ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 3s - loss: 1.1346 - accuracy: 0.6701 - 3s/epoch - 4ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 3s - loss: 1.1335 - accuracy: 0.6698 - 3s/epoch - 4ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 3s - loss: 1.1296 - accuracy: 0.6702 - 3s/epoch - 4ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 3s - loss: 1.1301 - accuracy: 0.6740 - 3s/epoch - 4ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 3s - loss: 1.1264 - accuracy: 0.6718 - 3s/epoch - 4ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 3s - loss: 1.1274 - accuracy: 0.6728 - 3s/epoch - 4ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 3s - loss: 1.1311 - accuracy: 0.6680 - 3s/epoch - 4ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 3s - loss: 1.1260 - accuracy: 0.6719 - 3s/epoch - 4ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 3s - loss: 1.1279 - accuracy: 0.6713 - 3s/epoch - 4ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 3s - loss: 1.1198 - accuracy: 0.6755 - 3s/epoch - 4ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 3s - loss: 1.1220 - accuracy: 0.6753 - 3s/epoch - 4ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 3s - loss: 1.1197 - accuracy: 0.6746 - 3s/epoch - 4ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 3s - loss: 1.1216 - accuracy: 0.6740 - 3s/epoch - 4ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 3s - loss: 1.1209 - accuracy: 0.6765 - 3s/epoch - 4ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 3s - loss: 1.1160 - accuracy: 0.6797 - 3s/epoch - 4ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 4s - loss: 1.1162 - accuracy: 0.6768 - 4s/epoch - 5ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 3s - loss: 1.1144 - accuracy: 0.6779 - 3s/epoch - 4ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 3s - loss: 1.1155 - accuracy: 0.6818 - 3s/epoch - 4ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 3s - loss: 1.1072 - accuracy: 0.6801 - 3s/epoch - 4ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 3s - loss: 1.1147 - accuracy: 0.6805 - 3s/epoch - 4ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 3s - loss: 1.1050 - accuracy: 0.6830 - 3s/epoch - 4ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 3s - loss: 1.1174 - accuracy: 0.6796 - 3s/epoch - 4ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 3s - loss: 1.1088 - accuracy: 0.6825 - 3s/epoch - 4ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 3s - loss: 1.1101 - accuracy: 0.6802 - 3s/epoch - 4ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 3s - loss: 1.1070 - accuracy: 0.6832 - 3s/epoch - 4ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 3s - loss: 1.1077 - accuracy: 0.6847 - 3s/epoch - 4ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 3s - loss: 1.1082 - accuracy: 0.6834 - 3s/epoch - 4ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 3s - loss: 1.1081 - accuracy: 0.6810 - 3s/epoch - 4ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 3s - loss: 1.1037 - accuracy: 0.6849 - 3s/epoch - 4ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 3s - loss: 1.1101 - accuracy: 0.6837 - 3s/epoch - 4ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 3s - loss: 1.0976 - accuracy: 0.6876 - 3s/epoch - 4ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 3s - loss: 1.1009 - accuracy: 0.6857 - 3s/epoch - 4ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 3s - loss: 1.1028 - accuracy: 0.6846 - 3s/epoch - 4ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 3s - loss: 1.0972 - accuracy: 0.6878 - 3s/epoch - 4ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 3s - loss: 1.0998 - accuracy: 0.6843 - 3s/epoch - 4ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 3s - loss: 1.1032 - accuracy: 0.6836 - 3s/epoch - 4ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 3s - loss: 1.0980 - accuracy: 0.6863 - 3s/epoch - 4ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 3s - loss: 1.1011 - accuracy: 0.6861 - 3s/epoch - 4ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 3s - loss: 1.1030 - accuracy: 0.6874 - 3s/epoch - 4ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 3s - loss: 1.1018 - accuracy: 0.6864 - 3s/epoch - 4ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 3s - loss: 1.0980 - accuracy: 0.6888 - 3s/epoch - 4ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 3s - loss: 1.0905 - accuracy: 0.6904 - 3s/epoch - 4ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 3s - loss: 1.0966 - accuracy: 0.6868 - 3s/epoch - 4ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 3s - loss: 1.0887 - accuracy: 0.6915 - 3s/epoch - 4ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 3s - loss: 1.0902 - accuracy: 0.6911 - 3s/epoch - 4ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 3s - loss: 1.0946 - accuracy: 0.6872 - 3s/epoch - 4ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 3s - loss: 1.0938 - accuracy: 0.6915 - 3s/epoch - 4ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 3s - loss: 1.0915 - accuracy: 0.6905 - 3s/epoch - 4ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 3s - loss: 1.0911 - accuracy: 0.6900 - 3s/epoch - 4ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 3s - loss: 1.0942 - accuracy: 0.6898 - 3s/epoch - 4ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 3s - loss: 1.0917 - accuracy: 0.6935 - 3s/epoch - 4ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 3s - loss: 1.0939 - accuracy: 0.6907 - 3s/epoch - 4ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 3s - loss: 1.0926 - accuracy: 0.6919 - 3s/epoch - 4ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 3s - loss: 1.0824 - accuracy: 0.6948 - 3s/epoch - 4ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 3s - loss: 1.0862 - accuracy: 0.6938 - 3s/epoch - 4ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 3s - loss: 1.0954 - accuracy: 0.6909 - 3s/epoch - 4ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 3s - loss: 1.0872 - accuracy: 0.6941 - 3s/epoch - 4ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 3s - loss: 1.0882 - accuracy: 0.6924 - 3s/epoch - 4ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 4s - loss: 1.0895 - accuracy: 0.6897 - 4s/epoch - 5ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 3s - loss: 1.0827 - accuracy: 0.6961 - 3s/epoch - 4ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 3s - loss: 1.0866 - accuracy: 0.6948 - 3s/epoch - 4ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 3s - loss: 1.0863 - accuracy: 0.6941 - 3s/epoch - 4ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 3s - loss: 1.0805 - accuracy: 0.6974 - 3s/epoch - 4ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 3s - loss: 1.0826 - accuracy: 0.6964 - 3s/epoch - 4ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 3s - loss: 1.0823 - accuracy: 0.6956 - 3s/epoch - 4ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 3s - loss: 1.0855 - accuracy: 0.6956 - 3s/epoch - 4ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 3s - loss: 1.0801 - accuracy: 0.6966 - 3s/epoch - 4ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 3s - loss: 1.0829 - accuracy: 0.6951 - 3s/epoch - 4ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 3s - loss: 1.0781 - accuracy: 0.6986 - 3s/epoch - 4ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 3s - loss: 1.0776 - accuracy: 0.6987 - 3s/epoch - 4ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 3s - loss: 1.0792 - accuracy: 0.6961 - 3s/epoch - 4ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 3s - loss: 1.0848 - accuracy: 0.6954 - 3s/epoch - 4ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 3s - loss: 1.0773 - accuracy: 0.6959 - 3s/epoch - 4ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 3s - loss: 1.0768 - accuracy: 0.6979 - 3s/epoch - 4ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 3s - loss: 1.0729 - accuracy: 0.7004 - 3s/epoch - 4ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 3s - loss: 1.0755 - accuracy: 0.6985 - 3s/epoch - 4ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 3s - loss: 1.0764 - accuracy: 0.6995 - 3s/epoch - 4ms/step\n",
            "157/157 - 1s - loss: 1.0965 - accuracy: 0.7230 - 549ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0965460538864136, 0.7229999899864197]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model = my_model()\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the new CNN from previous tutorial\n",
        "def define_my_model_3(give_summary=None):\n",
        "    \"\"\"\n",
        "      adding more layers\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "    x1 = layers.Conv2D(32, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    x1 = layers.BatchNormalization()(x1)\n",
        "    x1 = keras.activations.relu(x1)\n",
        "    x1 = layers.MaxPooling2D()(x1)\n",
        "\n",
        "    x2 = layers.Conv2D(64, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x1)\n",
        "    x2 = layers.BatchNormalization()(x2)\n",
        "    x2 = keras.activations.relu(x2)\n",
        "    # x2 = layers.MaxPooling2D()(x2)\n",
        "\n",
        "    x3 = layers.Conv2D(128, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x2)\n",
        "    x3 = layers.BatchNormalization()(x3)\n",
        "    x3 = keras.activations.relu(x3)\n",
        "    #x3 = layers.Flatten()(x3)\n",
        "\n",
        "    x4 = layers.Conv2D(256, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x3)\n",
        "    x4 = layers.BatchNormalization()(x4)\n",
        "    x4 = keras.activations.relu(x4)\n",
        "\n",
        "    x5 = layers.Conv2D(256, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))(x4)\n",
        "    x5 = layers.BatchNormalization()(x5)\n",
        "    x5 = keras.activations.relu(x5)\n",
        "    x5 = layers.Flatten()(x5)\n",
        "\n",
        "    # x5 = layers.Dense(2048, activation=\"relu\")(x4)\n",
        "    x6 = layers.Dense(256, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x5)\n",
        "    x7 = layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01))(x6)\n",
        "    x8 = layers.Dropout(0.5)(x7)\n",
        "    outputs = layers.Dense(10)(x8)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    if give_summary:\n",
        "      print(model.summary())\n",
        "    else:\n",
        "      pass\n",
        "    return model\n",
        "define_my_model_3(give_summary=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRTX2OSEHPJW",
        "outputId": "aaaae26d-bb38-42f3-8998-3451c01eb19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 32, 32, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_9 (TFOpLambda)   (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 16, 16, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " tf.nn.relu_10 (TFOpLambda)  (None, 16, 16, 64)        0         \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 16, 16, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16, 16, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " tf.nn.relu_11 (TFOpLambda)  (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 16, 16, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 16, 16, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " tf.nn.relu_12 (TFOpLambda)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 16, 16, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 16, 16, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " tf.nn.relu_13 (TFOpLambda)  (None, 16, 16, 256)       0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 65536)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               16777472  \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,793,098\n",
            "Trainable params: 17,791,626\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.functional.Functional at 0x7f8830527a90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_my_model_3()\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy1mW9zCOl8H",
        "outputId": "d812d9f6-960a-4563-f53b-6b93e9aa0842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 21s - loss: 5.4605 - accuracy: 0.3854 - 21s/epoch - 27ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 19s - loss: 2.1587 - accuracy: 0.5368 - 19s/epoch - 25ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 19s - loss: 1.7817 - accuracy: 0.6086 - 19s/epoch - 24ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 19s - loss: 1.5609 - accuracy: 0.6568 - 19s/epoch - 24ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 19s - loss: 1.4481 - accuracy: 0.6803 - 19s/epoch - 24ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 19s - loss: 1.3384 - accuracy: 0.7050 - 19s/epoch - 24ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 19s - loss: 1.2780 - accuracy: 0.7158 - 19s/epoch - 24ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 19s - loss: 1.2011 - accuracy: 0.7335 - 19s/epoch - 24ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 19s - loss: 1.1498 - accuracy: 0.7447 - 19s/epoch - 24ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 19s - loss: 1.1054 - accuracy: 0.7593 - 19s/epoch - 24ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 19s - loss: 1.0722 - accuracy: 0.7656 - 19s/epoch - 24ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 19s - loss: 1.0343 - accuracy: 0.7768 - 19s/epoch - 24ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 19s - loss: 1.0142 - accuracy: 0.7814 - 19s/epoch - 24ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 19s - loss: 0.9892 - accuracy: 0.7892 - 19s/epoch - 24ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 19s - loss: 0.9587 - accuracy: 0.7978 - 19s/epoch - 24ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 19s - loss: 0.9429 - accuracy: 0.7997 - 19s/epoch - 24ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 19s - loss: 0.9178 - accuracy: 0.8102 - 19s/epoch - 24ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 19s - loss: 0.9071 - accuracy: 0.8125 - 19s/epoch - 24ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 19s - loss: 0.8891 - accuracy: 0.8183 - 19s/epoch - 24ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 19s - loss: 0.8820 - accuracy: 0.8215 - 19s/epoch - 24ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 19s - loss: 0.8652 - accuracy: 0.8260 - 19s/epoch - 24ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 19s - loss: 0.8580 - accuracy: 0.8263 - 19s/epoch - 24ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 19s - loss: 0.8504 - accuracy: 0.8316 - 19s/epoch - 24ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 19s - loss: 0.8413 - accuracy: 0.8338 - 19s/epoch - 24ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 19s - loss: 0.8330 - accuracy: 0.8378 - 19s/epoch - 24ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 19s - loss: 0.8187 - accuracy: 0.8409 - 19s/epoch - 24ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 19s - loss: 0.8163 - accuracy: 0.8413 - 19s/epoch - 24ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 19s - loss: 0.8072 - accuracy: 0.8453 - 19s/epoch - 24ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 19s - loss: 0.8023 - accuracy: 0.8477 - 19s/epoch - 24ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 19s - loss: 0.8004 - accuracy: 0.8485 - 19s/epoch - 24ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 19s - loss: 0.7850 - accuracy: 0.8534 - 19s/epoch - 24ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 19s - loss: 0.7777 - accuracy: 0.8548 - 19s/epoch - 24ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 19s - loss: 0.7730 - accuracy: 0.8595 - 19s/epoch - 24ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 19s - loss: 0.7672 - accuracy: 0.8609 - 19s/epoch - 24ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 19s - loss: 0.7635 - accuracy: 0.8613 - 19s/epoch - 24ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 19s - loss: 0.7555 - accuracy: 0.8625 - 19s/epoch - 24ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 18s - loss: 0.7506 - accuracy: 0.8648 - 18s/epoch - 24ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 19s - loss: 0.7435 - accuracy: 0.8673 - 19s/epoch - 24ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 18s - loss: 0.7404 - accuracy: 0.8686 - 18s/epoch - 24ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 18s - loss: 0.7377 - accuracy: 0.8696 - 18s/epoch - 24ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 19s - loss: 0.7407 - accuracy: 0.8699 - 19s/epoch - 24ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 19s - loss: 0.7284 - accuracy: 0.8739 - 19s/epoch - 24ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 18s - loss: 0.7222 - accuracy: 0.8731 - 18s/epoch - 24ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 18s - loss: 0.7225 - accuracy: 0.8757 - 18s/epoch - 24ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 18s - loss: 0.7123 - accuracy: 0.8789 - 18s/epoch - 24ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 18s - loss: 0.7152 - accuracy: 0.8783 - 18s/epoch - 24ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 18s - loss: 0.7090 - accuracy: 0.8800 - 18s/epoch - 23ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 18s - loss: 0.6987 - accuracy: 0.8819 - 18s/epoch - 24ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 18s - loss: 0.7043 - accuracy: 0.8811 - 18s/epoch - 24ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 18s - loss: 0.6995 - accuracy: 0.8820 - 18s/epoch - 24ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 18s - loss: 0.6952 - accuracy: 0.8858 - 18s/epoch - 24ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 18s - loss: 0.6902 - accuracy: 0.8873 - 18s/epoch - 24ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 18s - loss: 0.6932 - accuracy: 0.8871 - 18s/epoch - 24ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 18s - loss: 0.6869 - accuracy: 0.8878 - 18s/epoch - 23ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 18s - loss: 0.6826 - accuracy: 0.8891 - 18s/epoch - 23ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 18s - loss: 0.6798 - accuracy: 0.8913 - 18s/epoch - 24ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 18s - loss: 0.6807 - accuracy: 0.8901 - 18s/epoch - 24ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 18s - loss: 0.6720 - accuracy: 0.8938 - 18s/epoch - 23ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 18s - loss: 0.6710 - accuracy: 0.8955 - 18s/epoch - 24ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 18s - loss: 0.6743 - accuracy: 0.8929 - 18s/epoch - 24ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 18s - loss: 0.6653 - accuracy: 0.8953 - 18s/epoch - 24ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 18s - loss: 0.6599 - accuracy: 0.8982 - 18s/epoch - 23ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 18s - loss: 0.6667 - accuracy: 0.8946 - 18s/epoch - 24ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 18s - loss: 0.6594 - accuracy: 0.8979 - 18s/epoch - 24ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 18s - loss: 0.6594 - accuracy: 0.8981 - 18s/epoch - 24ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 18s - loss: 0.6556 - accuracy: 0.8998 - 18s/epoch - 24ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 18s - loss: 0.6550 - accuracy: 0.8999 - 18s/epoch - 24ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 18s - loss: 0.6629 - accuracy: 0.8985 - 18s/epoch - 24ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 18s - loss: 0.6470 - accuracy: 0.9042 - 18s/epoch - 24ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 18s - loss: 0.6517 - accuracy: 0.9020 - 18s/epoch - 24ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 18s - loss: 0.6474 - accuracy: 0.9038 - 18s/epoch - 23ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 18s - loss: 0.6465 - accuracy: 0.9034 - 18s/epoch - 23ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 18s - loss: 0.6448 - accuracy: 0.9046 - 18s/epoch - 23ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 18s - loss: 0.6407 - accuracy: 0.9058 - 18s/epoch - 24ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 18s - loss: 0.6393 - accuracy: 0.9063 - 18s/epoch - 24ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 18s - loss: 0.6408 - accuracy: 0.9054 - 18s/epoch - 24ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 18s - loss: 0.6436 - accuracy: 0.9062 - 18s/epoch - 24ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 18s - loss: 0.6379 - accuracy: 0.9083 - 18s/epoch - 23ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 18s - loss: 0.6368 - accuracy: 0.9076 - 18s/epoch - 23ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 18s - loss: 0.6346 - accuracy: 0.9098 - 18s/epoch - 23ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 18s - loss: 0.6341 - accuracy: 0.9087 - 18s/epoch - 23ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 18s - loss: 0.6299 - accuracy: 0.9115 - 18s/epoch - 24ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 18s - loss: 0.6321 - accuracy: 0.9099 - 18s/epoch - 24ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 18s - loss: 0.6251 - accuracy: 0.9127 - 18s/epoch - 24ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 18s - loss: 0.6349 - accuracy: 0.9099 - 18s/epoch - 24ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 18s - loss: 0.6238 - accuracy: 0.9133 - 18s/epoch - 23ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 18s - loss: 0.6264 - accuracy: 0.9124 - 18s/epoch - 24ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 18s - loss: 0.6238 - accuracy: 0.9129 - 18s/epoch - 24ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 18s - loss: 0.6220 - accuracy: 0.9131 - 18s/epoch - 23ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 18s - loss: 0.6245 - accuracy: 0.9147 - 18s/epoch - 23ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 18s - loss: 0.6264 - accuracy: 0.9126 - 18s/epoch - 24ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 18s - loss: 0.6175 - accuracy: 0.9162 - 18s/epoch - 23ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 18s - loss: 0.6234 - accuracy: 0.9148 - 18s/epoch - 23ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 18s - loss: 0.6210 - accuracy: 0.9155 - 18s/epoch - 23ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 18s - loss: 0.6199 - accuracy: 0.9154 - 18s/epoch - 23ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 18s - loss: 0.6161 - accuracy: 0.9160 - 18s/epoch - 24ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 18s - loss: 0.6162 - accuracy: 0.9183 - 18s/epoch - 24ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 18s - loss: 0.6081 - accuracy: 0.9201 - 18s/epoch - 23ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 18s - loss: 0.6197 - accuracy: 0.9161 - 18s/epoch - 24ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 18s - loss: 0.6168 - accuracy: 0.9183 - 18s/epoch - 24ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 18s - loss: 0.6098 - accuracy: 0.9201 - 18s/epoch - 24ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 18s - loss: 0.6082 - accuracy: 0.9198 - 18s/epoch - 23ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 18s - loss: 0.6102 - accuracy: 0.9200 - 18s/epoch - 23ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 18s - loss: 0.6157 - accuracy: 0.9182 - 18s/epoch - 24ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 18s - loss: 0.6089 - accuracy: 0.9213 - 18s/epoch - 23ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 18s - loss: 0.6060 - accuracy: 0.9210 - 18s/epoch - 23ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 18s - loss: 0.6057 - accuracy: 0.9202 - 18s/epoch - 24ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 18s - loss: 0.6099 - accuracy: 0.9193 - 18s/epoch - 23ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 18s - loss: 0.6030 - accuracy: 0.9222 - 18s/epoch - 23ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 18s - loss: 0.6056 - accuracy: 0.9210 - 18s/epoch - 23ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 18s - loss: 0.6053 - accuracy: 0.9209 - 18s/epoch - 23ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 18s - loss: 0.6084 - accuracy: 0.9207 - 18s/epoch - 23ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 18s - loss: 0.6078 - accuracy: 0.9211 - 18s/epoch - 23ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 18s - loss: 0.6027 - accuracy: 0.9218 - 18s/epoch - 23ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 18s - loss: 0.6086 - accuracy: 0.9214 - 18s/epoch - 23ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 18s - loss: 0.5990 - accuracy: 0.9231 - 18s/epoch - 23ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 18s - loss: 0.6095 - accuracy: 0.9196 - 18s/epoch - 23ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 18s - loss: 0.5954 - accuracy: 0.9256 - 18s/epoch - 24ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 18s - loss: 0.6007 - accuracy: 0.9231 - 18s/epoch - 23ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 18s - loss: 0.5975 - accuracy: 0.9237 - 18s/epoch - 23ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 18s - loss: 0.5997 - accuracy: 0.9236 - 18s/epoch - 23ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 18s - loss: 0.5980 - accuracy: 0.9237 - 18s/epoch - 24ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 18s - loss: 0.5950 - accuracy: 0.9255 - 18s/epoch - 23ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 18s - loss: 0.6007 - accuracy: 0.9229 - 18s/epoch - 23ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 18s - loss: 0.5963 - accuracy: 0.9237 - 18s/epoch - 23ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 18s - loss: 0.5981 - accuracy: 0.9255 - 18s/epoch - 23ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 18s - loss: 0.5895 - accuracy: 0.9275 - 18s/epoch - 23ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 18s - loss: 0.5958 - accuracy: 0.9238 - 18s/epoch - 23ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 18s - loss: 0.5943 - accuracy: 0.9263 - 18s/epoch - 23ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 18s - loss: 0.5986 - accuracy: 0.9245 - 18s/epoch - 24ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 18s - loss: 0.5938 - accuracy: 0.9260 - 18s/epoch - 23ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 18s - loss: 0.5966 - accuracy: 0.9245 - 18s/epoch - 23ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 18s - loss: 0.5888 - accuracy: 0.9268 - 18s/epoch - 23ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 18s - loss: 0.5976 - accuracy: 0.9261 - 18s/epoch - 23ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 18s - loss: 0.5952 - accuracy: 0.9255 - 18s/epoch - 23ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 18s - loss: 0.5933 - accuracy: 0.9254 - 18s/epoch - 23ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 18s - loss: 0.5981 - accuracy: 0.9263 - 18s/epoch - 23ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 18s - loss: 0.5985 - accuracy: 0.9253 - 18s/epoch - 23ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 18s - loss: 0.5913 - accuracy: 0.9281 - 18s/epoch - 23ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 18s - loss: 0.5881 - accuracy: 0.9272 - 18s/epoch - 23ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 18s - loss: 0.5873 - accuracy: 0.9282 - 18s/epoch - 23ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 18s - loss: 0.5889 - accuracy: 0.9272 - 18s/epoch - 23ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 18s - loss: 0.5868 - accuracy: 0.9282 - 18s/epoch - 23ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 18s - loss: 0.5857 - accuracy: 0.9280 - 18s/epoch - 23ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 18s - loss: 0.5868 - accuracy: 0.9284 - 18s/epoch - 24ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 18s - loss: 0.5968 - accuracy: 0.9279 - 18s/epoch - 23ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 18s - loss: 0.5823 - accuracy: 0.9313 - 18s/epoch - 23ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 18s - loss: 0.5844 - accuracy: 0.9292 - 18s/epoch - 23ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 18s - loss: 0.5912 - accuracy: 0.9277 - 18s/epoch - 23ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 18s - loss: 0.5840 - accuracy: 0.9295 - 18s/epoch - 23ms/step\n",
            "157/157 - 1s - loss: 1.2205 - accuracy: 0.7687 - 1s/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2205371856689453, 0.7687000036239624]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MTXyLGYzOxEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}