{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-BZcRAVS4EA",
        "outputId": "2a7311e8-6a35-495e-8efb-d470dc32a198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "\n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjDdl4FuS43i",
        "outputId": "40212b8f-d94a-414d-83f2-eb40cb737bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "6.55238884100001\n",
            "GPU (s):\n",
            "0.08951229400000216\n",
            "GPU speedup over CPU: 73x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkQ3-MxY2_MF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functional API for DoubleDigit Classification (in-depth)"
      ],
      "metadata": {
        "id": "ArYnGZCZ8g54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # HYPERPARAMETERS\n",
        "# BATCH_SIZE = 64\n",
        "# WEIGHT_DECAY = 0.001\n",
        "# LEARNING_RATE = 0.001\n",
        "\n",
        "# train_df = pd.read_csv(\"train.csv\")\n",
        "# test_df = pd.read_csv(\"test.csv\")\n",
        "# train_images = os.getcwd() + \"/train_images/\" + train_df.iloc[:, 0].values\n",
        "# test_images = os.getcwd() + \"/test_images/\" + test_df.iloc[:, 0].values\n",
        "\n",
        "# train_labels = train_df.iloc[:, 1:].values\n",
        "# test_labels = test_df.iloc[:, 1:].values\n",
        "\n",
        "# def read_image(image_path, label):\n",
        "#     image = tf.io.read_file(image_path)\n",
        "#     image = tf.image.decode_image(image, channels=1, dtype=tf.float32)\n",
        "\n",
        "#     # In older versions you need to set shape in order to avoid error\n",
        "#     # on newer (2.3.0+) the following 3 lines can safely be removed\n",
        "#     image.set_shape((64, 64, 1))\n",
        "#     label[0].set_shape([])\n",
        "#     label[1].set_shape([])\n",
        "\n",
        "#     labels = {\"first_num\": label[0], \"second_num\": label[1]}\n",
        "#     return image, labels\n",
        "\n",
        "\n",
        "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "# train_dataset = (\n",
        "#     train_dataset.shuffle(buffer_size=len(train_labels))\n",
        "#     .map(read_image)\n",
        "#     .batch(batch_size=BATCH_SIZE)\n",
        "#     .prefetch(buffer_size=AUTOTUNE)\n",
        "# )\n",
        "\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "# test_dataset = (\n",
        "#     test_dataset.map(read_image)\n",
        "#     .batch(batch_size=BATCH_SIZE)\n",
        "#     .prefetch(buffer_size=AUTOTUNE)\n",
        "# )"
      ],
      "metadata": {
        "id": "b1yfiNA736X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = keras.Input(shape=(64, 64, 1))\n",
        "# x = layers.Conv2D(\n",
        "#     filters=32,\n",
        "#     kernel_size=3,\n",
        "#     padding=\"same\",\n",
        "#     kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "# )(inputs)\n",
        "# x = layers.BatchNormalization()(x)\n",
        "# x = keras.activations.relu(x)\n",
        "\n",
        "# x = layers.Conv2D(64, 3, kernel_regularizer=regularizers.l2(WEIGHT_DECAY),)(x)\n",
        "# x = layers.BatchNormalization()(x)\n",
        "# x = keras.activations.relu(x)\n",
        "# x = layers.MaxPooling2D()(x)\n",
        "\n",
        "# x = layers.Conv2D(\n",
        "#     64, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        "# )(x)\n",
        "\n",
        "# x = layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D()(x)\n",
        "\n",
        "# x = layers.Flatten()(x)\n",
        "# x = layers.Dense(128, activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.5)(x)\n",
        "# x = layers.Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "# Because of 2 outputs Functional API makes it very easy to implement\n",
        "\n",
        "# output1 = layers.Dense(10, activation=\"softmax\", name=\"first_num\")(x)\n",
        "# output2 = layers.Dense(10, activation=\"softmax\", name=\"second_num\")(x)\n",
        "\n",
        "# model = keras.Model(inputs=inputs, outputs=[output1, output2])\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "#     loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "#     metrics=[\"accuracy\"],\n",
        "# )\n",
        "\n",
        "# model.fit(train_dataset, epochs=5, verbose=2)\n",
        "# model.evaluate(test_dataset, verbose=2)"
      ],
      "metadata": {
        "id": "Ks80AsMu5uOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Subclassing with Keras"
      ],
      "metadata": {
        "id": "dvqwAhPH9JAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test,y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZ5wR936H0o",
        "outputId": "25ab98be-b0a2-489c-d6bf-78de5eb06b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "(10000, 28, 28, 1)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each layer,  CNN -> BatchNorm -> ReLU (common structure)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "x10 (a lot of code to write!)\n",
        "\n",
        "So we create a subclass to directly add a layer without having to type so much\n"
      ],
      "metadata": {
        "id": "5ooDQeUq9qqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN Block SubClass"
      ],
      "metadata": {
        "id": "CsIXbDP5RInE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(layers.Layer):\n",
        "    def __init__(self, out_channels, kernel_size=3):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = layers.Conv2D(out_channels, kernel_size, padding=\"same\")\n",
        "        self.bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        x = self.conv(input_tensor)\n",
        "        print(x.shape)\n",
        "        x = self.bn(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [CNNBlock(32),\n",
        "     CNNBlock(64),\n",
        "     CNNBlock(128),\n",
        "     layers.Flatten(),\n",
        "     layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "#print(model.summary())\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnPoMYm09miZ",
        "outputId": "f794be92-14ca-4c1b-8707-56a60cee5130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "(None, 28, 28, 32)\n",
            "(None, 28, 28, 64)\n",
            "(None, 28, 28, 128)\n",
            "(None, 28, 28, 32)\n",
            "(None, 28, 28, 64)\n",
            "(None, 28, 28, 128)\n",
            "(None, 28, 28, 32)\n",
            "(None, 28, 28, 64)\n",
            "(None, 28, 28, 128)\n",
            "938/938 - 22s - loss: 0.5381 - accuracy: 0.9479 - 22s/epoch - 23ms/step\n",
            "Epoch 2/5\n",
            "938/938 - 21s - loss: 0.0833 - accuracy: 0.9831 - 21s/epoch - 22ms/step\n",
            "Epoch 3/5\n",
            "938/938 - 21s - loss: 0.0341 - accuracy: 0.9897 - 21s/epoch - 23ms/step\n",
            "Epoch 4/5\n",
            "938/938 - 21s - loss: 0.0267 - accuracy: 0.9915 - 21s/epoch - 22ms/step\n",
            "Epoch 5/5\n",
            "938/938 - 21s - loss: 0.0266 - accuracy: 0.9914 - 21s/epoch - 23ms/step\n",
            "(None, 28, 28, 32)\n",
            "(None, 28, 28, 64)\n",
            "(None, 28, 28, 128)\n",
            "157/157 - 1s - loss: 0.0539 - accuracy: 0.9849 - 1s/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05394566059112549, 0.9848999977111816]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResBlock SubClass"
      ],
      "metadata": {
        "id": "g6LnMiW9WDbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(layers.Layer):\n",
        "    def __init__(self, channels):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.cnn1 = CNNBlock(channels[0], 3)\n",
        "        self.cnn2 = CNNBlock(channels[1], 3)\n",
        "        self.cnn3 = CNNBlock(channels[2], 3)\n",
        "        self.pooling = layers.MaxPooling2D()\n",
        "        self.identity_mapping = layers.Conv2D(channels[1], 3, padding=\"same\")\n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        x = self.cnn1(input_tensor, training=training)\n",
        "        x = self.cnn2(x, training=training)\n",
        "        x = self.cnn3(x + self.identity_mapping(input_tensor), training=training,)\n",
        "        x = self.pooling(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "moYuUSiQRbYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}